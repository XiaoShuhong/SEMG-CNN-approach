{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Model with features.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AQ_FmI-9JXEa"
      },
      "source": [
        "Load Library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ezrGmE0ZJMts"
      },
      "source": [
        "from keras.applications.xception import Xception\n",
        "from keras.preprocessing import image\n",
        "import numpy as np\n",
        "from keras.layers import Dense, Dropout\n",
        "from keras.models import Model\n",
        "import os\n",
        "from keras.preprocessing import image\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from keras.metrics import categorical_accuracy\n",
        "from keras.callbacks import Callback, ModelCheckpoint,LearningRateScheduler,EarlyStopping,ReduceLROnPlateau\n",
        "from keras.optimizers import SGD,Adam"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ffw9SwVnJhF3"
      },
      "source": [
        "Load Driver"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UJfHIzzSJRsK",
        "outputId": "35f7b571-4777-41a4-923f-bddf45395a3f"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4gYnQ8YJkLn"
      },
      "source": [
        "Load data set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gUY6AaX2JUKR"
      },
      "source": [
        "##load data set for subject s with feature fea\n",
        "##features to choose are feature_list=['MAV','WL','SSC','ZC','IEMG','VAR','WAMP','RMS','mDWT','HEMG','MAVS','ARC','MNF','PSR','SampEn','CC','HHT']\n",
        "def Load_data(s,fea):\n",
        "  if s<0 or s>27:\n",
        "    print('subject not exist')\n",
        "    return\n",
        "  else:\n",
        "    path='/content/drive/MyDrive/srtp-xsh/RGBimg_with_feature/s'+str(s)+'/'+str(fea)\n",
        "    s_weight='/content/drive/MyDrive/srtp-xsh/RGBimg_with_feature/s'+str(s)+'/'+str(fea)+'/weight.h5'\n",
        "    train=path+'/'+os.listdir(path)[1]\n",
        "    test=path+'/'+os.listdir(path)[0]\n",
        "    X_train=[]\n",
        "    Y_train=[]\n",
        "    for x in os.listdir(train):\n",
        "\n",
        "      f=train+'/'+x\n",
        "      img=image.load_img(f)\n",
        "      img=image.img_to_array(img)\n",
        "      img=image.smart_resize(img, (100,200), interpolation='bilinear')\n",
        "      label=x[3]\n",
        "      X_train.append(img)\n",
        "      Y_train.append(label)\n",
        "    X_test=[]\n",
        "    Y_test=[]\n",
        "    for x in os.listdir(test):\n",
        "      f=test+'/'+x\n",
        "      img=image.load_img(f)\n",
        "      img=image.img_to_array(img)\n",
        "      img=image.smart_resize(img, (100,200), interpolation='bilinear')\n",
        "      label=x[3]\n",
        "      X_test.append(img)\n",
        "      Y_test.append(label)\n",
        "    X_train=np.asarray(X_train,np.float32)\n",
        "    X_test=np.asarray(X_test,np.float32)\n",
        "    Y_train=np.asarray(Y_train,np.float32)\n",
        "    Y_test=np.asarray(Y_test,np.float32)\n",
        "\n",
        "    num_example=X_train.shape[0]\n",
        "    arr=np.arange(num_example)\n",
        "    np.random.shuffle(arr)\n",
        "    X_train=X_train[arr]\n",
        "    Y_train=Y_train[arr]\n",
        "    Y_train = to_categorical(Y_train,num_classes=52)\n",
        "    Y_test = to_categorical(Y_test,num_classes=52)\n",
        "    print('successfully load data for subject',str(s),'with feature',fea)\n",
        "    print('shape of training data:',X_train.shape)\n",
        "    print('shape of training label:',Y_train.shape)\n",
        "    print('shape of test data:',X_test.shape)\n",
        "    print('shape of test label:',Y_test.shape)\n",
        "    return X_train,Y_train,X_test,Y_test,s_weight,path"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 307
        },
        "id": "5Ip-bNdvLck1",
        "outputId": "ada7a6cf-167e-46a3-a4dd-9923ed5b2bea"
      },
      "source": [
        "X_train,Y_train,X_test,Y_test,s_weight,path=Load_data(2,'IEMG')"
      ],
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-144-49b27a534bd1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ms_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mLoad_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'IEMG'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-15-41f1a152f1cf>\u001b[0m in \u001b[0;36mLoad_data\u001b[0;34m(s, fea)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m       \u001b[0mf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'/'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m       \u001b[0mimg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_img\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m       \u001b[0mimg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimg_to_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m       \u001b[0mimg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msmart_resize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterpolation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'bilinear'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/preprocessing/image.py\u001b[0m in \u001b[0;36mload_img\u001b[0;34m(path, grayscale, color_mode, target_size, interpolation)\u001b[0m\n\u001b[1;32m    291\u001b[0m   \"\"\"\n\u001b[1;32m    292\u001b[0m   return image.load_img(path, grayscale=grayscale, color_mode=color_mode,\n\u001b[0;32m--> 293\u001b[0;31m                         target_size=target_size, interpolation=interpolation)\n\u001b[0m\u001b[1;32m    294\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras_preprocessing/image/utils.py\u001b[0m in \u001b[0;36mload_img\u001b[0;34m(path, grayscale, color_mode, target_size, interpolation)\u001b[0m\n\u001b[1;32m    112\u001b[0m                           'The use of `load_img` requires PIL.')\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpil_image\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcolor_mode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'grayscale'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m             \u001b[0;31m# if image is not already an 8-bit, 16-bit or 32-bit grayscale image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RwSFOKEfbhni"
      },
      "source": [
        "Load model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ifgEB8SLiFt"
      },
      "source": [
        "\n",
        "base_model=Xception(include_top=False, weights='imagenet', input_tensor=None, input_shape=(100,200,3), pooling='avg')\n",
        "x=base_model.output\n",
        "x = Dense(1024, activation='relu')(x)\n",
        "x = Dense(512, activation='relu')(x)\n",
        "predictions = Dense(52, activation='softmax')(x)\n",
        "\n",
        "model = Model(inputs=base_model.input, outputs=predictions)"
      ],
      "execution_count": 141,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DqnPpTNtbjcR"
      },
      "source": [
        "overview of performance(skip this part if tuning is required)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "88bPeWfXbe9Q",
        "outputId": "94126ebe-0f51-4a2c-bbf7-ca86c190fb50"
      },
      "source": [
        "opt=Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-07, amsgrad=False,name='Adam')\n",
        "learning_rate_reduction = ReduceLROnPlateau(monitor='val_loss', patience=3, verbose=1, factor=0.5, min_lr=0.00001)\n",
        "model.compile(optimizer=opt, loss='categorical_crossentropy',metrics=['categorical_accuracy']) \n",
        "callbacks = [ModelCheckpoint(s_weight, monitor='val_loss', save_best_only=True),learning_rate_reduction]\n",
        "history = model.fit(         X_train,\n",
        "                    Y_train,\n",
        "                    batch_size=32,\n",
        "                    epochs=24,\n",
        "                    validation_split=0.1,\n",
        "                    callbacks=callbacks)"
      ],
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/24\n",
            "11/11 [==============================] - 15s 508ms/step - loss: 2.8556 - categorical_accuracy: 0.1744 - val_loss: 7.6226 - val_categorical_accuracy: 0.1892\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
            "  category=CustomMaskWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 2/24\n",
            "11/11 [==============================] - 3s 239ms/step - loss: 1.4193 - categorical_accuracy: 0.5497 - val_loss: 27.1352 - val_categorical_accuracy: 0.1892\n",
            "Epoch 3/24\n",
            "11/11 [==============================] - 3s 244ms/step - loss: 1.4380 - categorical_accuracy: 0.6189 - val_loss: 65.9275 - val_categorical_accuracy: 0.0811\n",
            "Epoch 4/24\n",
            "11/11 [==============================] - 3s 243ms/step - loss: 0.8239 - categorical_accuracy: 0.7007 - val_loss: 17.4133 - val_categorical_accuracy: 0.0811\n",
            "\n",
            "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "Epoch 5/24\n",
            "11/11 [==============================] - 3s 255ms/step - loss: 0.4270 - categorical_accuracy: 0.8497 - val_loss: 11.5605 - val_categorical_accuracy: 0.2432\n",
            "Epoch 6/24\n",
            "11/11 [==============================] - 3s 247ms/step - loss: 0.2948 - categorical_accuracy: 0.9018 - val_loss: 4.5900 - val_categorical_accuracy: 0.2973\n",
            "Epoch 7/24\n",
            "11/11 [==============================] - 3s 242ms/step - loss: 0.2440 - categorical_accuracy: 0.9164 - val_loss: 6.7154 - val_categorical_accuracy: 0.2432\n",
            "Epoch 8/24\n",
            "11/11 [==============================] - 3s 248ms/step - loss: 0.1658 - categorical_accuracy: 0.9512 - val_loss: 7.2545 - val_categorical_accuracy: 0.2973\n",
            "Epoch 9/24\n",
            "11/11 [==============================] - 3s 241ms/step - loss: 0.1027 - categorical_accuracy: 0.9447 - val_loss: 7.3533 - val_categorical_accuracy: 0.3514\n",
            "\n",
            "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "Epoch 10/24\n",
            "11/11 [==============================] - 3s 245ms/step - loss: 0.0822 - categorical_accuracy: 0.9618 - val_loss: 5.3428 - val_categorical_accuracy: 0.2973\n",
            "Epoch 11/24\n",
            "11/11 [==============================] - 3s 240ms/step - loss: 0.0618 - categorical_accuracy: 0.9778 - val_loss: 4.4211 - val_categorical_accuracy: 0.3784\n",
            "Epoch 12/24\n",
            "11/11 [==============================] - 3s 236ms/step - loss: 0.1405 - categorical_accuracy: 0.9582 - val_loss: 3.2310 - val_categorical_accuracy: 0.3784\n",
            "Epoch 13/24\n",
            "11/11 [==============================] - 3s 235ms/step - loss: 0.0705 - categorical_accuracy: 0.9775 - val_loss: 3.3187 - val_categorical_accuracy: 0.4324\n",
            "Epoch 14/24\n",
            "11/11 [==============================] - 3s 235ms/step - loss: 0.0491 - categorical_accuracy: 0.9878 - val_loss: 2.6777 - val_categorical_accuracy: 0.4054\n",
            "Epoch 15/24\n",
            "11/11 [==============================] - 3s 235ms/step - loss: 0.1333 - categorical_accuracy: 0.9686 - val_loss: 2.8230 - val_categorical_accuracy: 0.3784\n",
            "Epoch 16/24\n",
            "11/11 [==============================] - 3s 235ms/step - loss: 0.0638 - categorical_accuracy: 0.9822 - val_loss: 2.5849 - val_categorical_accuracy: 0.3784\n",
            "Epoch 17/24\n",
            "11/11 [==============================] - 3s 236ms/step - loss: 0.0178 - categorical_accuracy: 0.9989 - val_loss: 2.7154 - val_categorical_accuracy: 0.4595\n",
            "Epoch 18/24\n",
            "11/11 [==============================] - 3s 236ms/step - loss: 0.0267 - categorical_accuracy: 0.9919 - val_loss: 2.8437 - val_categorical_accuracy: 0.5135\n",
            "Epoch 19/24\n",
            "11/11 [==============================] - 3s 239ms/step - loss: 0.0149 - categorical_accuracy: 0.9985 - val_loss: 2.9874 - val_categorical_accuracy: 0.5135\n",
            "\n",
            "Epoch 00019: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "Epoch 20/24\n",
            "11/11 [==============================] - 3s 237ms/step - loss: 0.0098 - categorical_accuracy: 0.9978 - val_loss: 3.1670 - val_categorical_accuracy: 0.5405\n",
            "Epoch 21/24\n",
            "11/11 [==============================] - 3s 235ms/step - loss: 0.0209 - categorical_accuracy: 0.9990 - val_loss: 3.0621 - val_categorical_accuracy: 0.5405\n",
            "Epoch 22/24\n",
            "11/11 [==============================] - 3s 234ms/step - loss: 0.0094 - categorical_accuracy: 0.9989 - val_loss: 3.0000 - val_categorical_accuracy: 0.4865\n",
            "\n",
            "Epoch 00022: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
            "Epoch 23/24\n",
            "11/11 [==============================] - 3s 235ms/step - loss: 0.0214 - categorical_accuracy: 0.9966 - val_loss: 2.9203 - val_categorical_accuracy: 0.4865\n",
            "Epoch 24/24\n",
            "11/11 [==============================] - 3s 235ms/step - loss: 0.0070 - categorical_accuracy: 1.0000 - val_loss: 2.8518 - val_categorical_accuracy: 0.4865\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jnrHq1WIbrSM",
        "outputId": "7afef874-2dec-4ce6-d935-289cf2c22f95"
      },
      "source": [
        "model.load_weights(s_weight)\n",
        "loss, accuracy = model.evaluate(X_test,\n",
        "                  Y_test)\n",
        "print('loss = {0:.4f}, accuracy = {1:.4f}'.format(loss, accuracy))"
      ],
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5/5 [==============================] - 0s 71ms/step - loss: 2.1198 - categorical_accuracy: 0.5385\n",
            "loss = 2.1198, accuracy = 0.5385\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1_WS5fHFf-rW"
      },
      "source": [
        ""
      ],
      "execution_count": 143,
      "outputs": []
    }
  ]
}